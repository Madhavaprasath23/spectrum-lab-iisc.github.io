<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://spectrum-lab-iisc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://spectrum-lab-iisc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-07T13:20:42+00:00</updated><id>https://spectrum-lab-iisc.github.io/feed.xml</id><subtitle>Spectrum Lab</subtitle><entry><title type="html">The Role of the Data-Fidelity Term in Solving Linear Inverse Problems</title><link href="https://spectrum-lab-iisc.github.io/blog/2025/tight/" rel="alternate" type="text/html" title="The Role of the Data-Fidelity Term in Solving Linear Inverse Problems"/><published>2025-08-06T00:00:00+00:00</published><updated>2025-08-06T00:00:00+00:00</updated><id>https://spectrum-lab-iisc.github.io/blog/2025/tight</id><content type="html" xml:base="https://spectrum-lab-iisc.github.io/blog/2025/tight/"><![CDATA[<div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0 blog-ready"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tight/alt_prox.jpg" sizes="95vw"/> <img src="/assets/img/tight/alt_prox.jpg" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="the-data-fidelity-term-in-linear-inverse-problems">The Data-Fidelity Term in Linear Inverse Problems</h2> <p class="text-justify">In some of our recent works, we investigate the role of the data-fidelity term in solving linear inverse problems. Most often in linear inverse problems, the $\ell_2$-norm is used to compute the error between the measurements and the measurement model. Consider the measurement model:</p> <div style="text-align: center;"> $y = Ax + w.$ <br/><br/> </div> <p class="text-justify">Here, $A$ is the forward operator (or sensing matrix) that describes the measurement process, and $w$ represents measurement noise. Since $A$ is often ill-conditioned or rectangular (with fewer measurements than signal dimensions), the problem is ill-posed and requires regularization to find a meaningful solution.</p> <p class="text-justify">The standard approach is to solve an optimization problem with the the $\ell_2$-norm as the data-fidelity metric:</p> <div style="text-align: center;"> $\min_{x} \frac{1}{2} \|Ax - y\|_2^2 + \lambda g(x)$ <br/><br/> </div> <p class="text-justify">Although this choice has statistical backing, i.e., the $\ell_2$-norm is a direct consequence of modelling additive noise as a Gaussian random variable in maximum a posteriori estimation, it is often not the best choice in practice <d-cite key="gribonval2021bayesian"></d-cite>.</p> <h2 id="introduction-to-frames">Introduction to Frames</h2> <p class="text-justify">In linear algebra, a basis for a vector space is a set of linearly independent vectors that span the space. This means any vector in the space can be written as a <em>unique</em> linear combination of the basis vectors. Frames are a generalization of bases that provide more flexibility. A frame is a set of vectors that also spans the space but is not required to be linearly independent. This redundancy can be highly beneficial for robustness to noise and errors.</p> <p class="text-justify">Formally, a set of $N$ vectors $\lbrace v_i \in \mathbb{R}^n \rbrace_{i=1}^N$ is said to constitute a <strong>frame</strong> for $\mathbb{R}^n$ if there exist constants $0 &lt; \alpha \le \beta &lt; \infty$ such that for any vector $x \in \mathbb{R}^n$, the following condition holds:</p> <div style="text-align: center;"> $\alpha \|x\|_2^2 \le \sum_{i=1}^{N} |\langle x, v_i \rangle|^2 \le \beta \|x\|_2^2$ <br/><br/> </div> <p>The constants $\alpha$ and $\beta$ are called the frame bounds.</p> <p class="text-justify">If the frame bounds are equal, $\alpha = \beta$, the frame is called a <strong>tight frame</strong>. If $\alpha = \beta = 1$, it is a <strong>Parseval tight frame</strong>. Tight frames share some of the desirable properties of orthonormal bases, making them particularly useful in areas like compressed sensing.</p> <h2 id="the-back-projection-loss-as-a-distance-minimization">The Back-Projection Loss as a Distance Minimization</h2> <p class="text-justify">The back-projection loss is defined as the squared distance from a candidate solution $x$ to the solution space of the measurement model. Let’s define the solution space (in the noiseless case) as the affine subspace $C = \lbrace z \in \mathbb{R}^n : Az = y\rbrace$. This set contains all possible signals that are perfectly consistent with the measurements $y$.</p> <p>The squared Euclidean distance from a point $x$ to this set is given by:</p> <div style="text-align: center;"> $d_C^2(x) = \min_{z \in C} \|x - z\|_2^2$ <br/><br/> </div> <p class="text-justify">The point in $C$ closest to $x$ is the orthogonal projection of $x$ onto $C$, denoted as $\text{proj}_C(x)$. For a full row-rank matrix $A$, this projection is given by:</p> <div style="text-align: center;"> $\text{proj}_C(x) = x - A^{\dagger}(Ax - y)$ <br/><br/> </div> <p class="text-justify">where $A^{\dagger} = A^T(AA^T)^{-1}$ is the Moore-Penrose pseudoinverse of $A$. Therefore, the squared distance is:</p> <div style="text-align: center;"> $d_C^2(x) = \|x - \text{proj}_C(x)\|_2^2 = \|A^{\dagger}(Ax - y)\|_2^2$ <br/><br/> </div> <p class="text-justify">The back-projection loss is precisely this squared distance. The optimization problem then becomes:</p> <div style="text-align: center;"> $\min_{x} \frac{1}{2} \|A^{\dagger}(y - Ax)\|_2^2 + \lambda g(x)$ <br/><br/> </div> <p class="text-justify">This formulation seeks a solution that is not only regularized by $g(x)$ but is also minimally distant from the space of all signals that could have produced the measurements. Such a choice has been shown to provide superior reconstruction accuracy for inverse problems in imaging <d-cite key="tirer2021convergence"></d-cite>, compressive sensing <d-cite key="nareddy2024tight"></d-cite>, and finite-rate-of-innovation signal reconstruction <d-cite key="kamath2025deepfri"></d-cite>.</p> <h2 id="back-projection-and-tight-frames-in-compressed-sensing">Back-Projection and Tight Frames in Compressed Sensing</h2> <p class="text-justify">In compressed sensing (CS), the goal is to recover a sparse signal from a few measurements. The choice of the sensing matrix $A$ is critical. While random Gaussian matrices are easy to construct and satisfy theoretical guarantees like the Restricted Isometry Property (RIP), <strong>tight-frame</strong> matrices are known to yield the minimum mean-squared error. A matrix $V$ is a Parseval tight frame if $VV^\top = I$.</p> <h3 id="minimum-mse-recovery-with-an-oracle">Minimum MSE Recovery with an Oracle</h3> <p class="text-justify">To understand why tight frames are optimal, consider an idealized scenario where an “oracle” tells us the exact locations (the support, denoted by $S$) of the non-zero entries in the sparse signal $x$. The recovery problem then simplifies to finding the <em>values</em> of these non-zero coefficients.</p> <p class="text-justify">Let $x_S$ be the vector of non-zero values of $x$ and $A_S$ be the submatrix of $A$ containing only the columns indexed by the support $S$. The measurement model becomes:</p> <div style="text-align: center;"> $y = A_S x_S + w$ <br/><br/> </div> <p class="text-justify">This is a standard linear estimation problem. The best linear unbiased estimator for $x_S$ is the least-squares solution, which gives the recovered values $\hat{x}_S$:</p> <div style="text-align: center;"> $\hat{x}_S = (A_S^T A_S)^{-1} A_S^T y$. <br/><br/> </div> <p class="text-justify">The mean-squared error (MSE) of this estimate is directly related to the properties of the matrix $(A_S^T A_S)^{-1}$. The error is minimized when this matrix is well-conditioned. A <strong>tight-frame</strong> sensing matrix $A$ ensures that for any support $S$, the columns of $A_S$ are as close to orthogonal as possible, which makes $A_S^T A_S$ well-conditioned (close to the identity matrix). This minimizes the amplification of the noise $w$ during the recovery process, thus achieving the minimum possible MSE.</p> <h3 id="the-back-projection-bridge">The Back-Projection Bridge</h3> <p class="text-justify">The back-projection loss provides a remarkable bridge between the convenience of random matrices and the optimality of tight frames. Consider the standard CS problem formulation with a non-tight sensing matrix $A$ (e.g., Gaussian) and the back-projection loss:</p> <div style="text-align: center;"> $\min_{x} \|D^\top x\|_1 \quad \text{subject to} \quad \|A^{\dagger}(y - Ax)\|_2 \le \epsilon$, <br/><br/> </div> <p>where $D$ is a sparsifying dictionary. This is equivalent to solving the problem with a modified data-fidelity term $\Vert B(Ax-y)\Vert^2_2$ where $B=(AA^T)^{-1/2}$.</p> <p class="text-justify">This formulation is equivalent to solving the original problem with an <em>effective</em> sensing matrix $\tilde{A} = BA = (AA^T)^{-1/2}A$ and effective measurements $\tilde{y} = By$. The key insight is that this new sensing matrix $\tilde{A}$ is a <strong>Parseval tight frame</strong>, because:</p> <div style="text-align: center;"> $\tilde{A}\tilde{A}^T = (BA)(BA)^T = BAA^TB^T = (AA^T)^{-1/2}AA^T(AA^T)^{-1/2} = I$. <br/><br/> </div> <p class="text-justify">This means that by simply changing the data-fidelity loss to the back-projection loss, we can gain the performance benefits of a tight-frame sensing matrix without the difficulty of constructing one. This leads to improved recovery guarantees, especially when the number of measurements is low or the signal is less sparse.</p> <h2 id="a-wiener-filter-like-update-for-image-deconvolution">A Wiener-Filter-like Update for Image Deconvolution</h2> <p class="text-justify">When the forward model $A$ represents a convolution, such as in image deblurring, the back-projection loss has another elegant interpretation. In iterative optimization algorithms like the proximal gradient method, the update step involves computing the gradient of the data-fidelity term.</p> <p class="text-justify">For the standard least-squares loss, the gradient is $\nabla f_{LS}(x) = A^T(Ax-y)$. For the back-projection loss, the gradient is $\nabla f_{BP}(x) = A^T(AA^T)^{-1}(Ax-y)$.</p> <p class="text-justify">In practice, the matrix $AA^T$ can be ill-conditioned, so a regularized inverse is used. For a convolutional operator $H$, the gradient update becomes:</p> <div style="text-align: center;"> $\nabla f_{BP}(x) = H^T(HH^T + \epsilon I)^{-1}(Hx - y)$ <br/><br/> </div> <p class="text-justify">This expression is precisely the form of a <strong>Wiener filter</strong>. In the Fourier domain, if $\mathcal{F}(\cdot)$ is the Fourier transform, the operator $H^T(HH^T + \epsilon I)^{-1}$ becomes a filter with the frequency response:</p> <div style="text-align: center;"> $\displaystyle\frac{\overline{\mathcal{F}(h)}}{|\mathcal{F}(h)|^2 + \epsilon}$, <br/><br/> </div> <p class="text-justify">where $h$ is the blur kernel. This filter adaptively de-emphasizes frequencies that were attenuated by the blur, preventing noise amplification. Therefore, using the back-projection loss in a convolutional setting is akin to incorporating a Wiener-filter-like deconvolution step directly into the gradient update of the optimization, leading to more stable and accurate reconstructions.</p> <h2 id="solving-inverse-problems-with-the-method-of-alternating-proximations">Solving Inverse Problems with the Method of Alternating Proximations</h2> <p>This optimization problem can be solved efficiently using the <strong>proximal gradient method (PGM)</strong>. PGM is an iterative algorithm designed for problems that are a sum of a smooth function and a (possibly non-smooth) function for which we can compute a proximal operator.</p> <p>In our case, the objective function is $f(x) + h(x)$, where $f(x) = \frac{1}{2} \Vert A^{\dagger}(y - Ax)\Vert_2^2$ is the smooth data-fidelity term, and $h(x) = \lambda g(x)$ is the regularization term.</p> <p>The gradient of the smooth part is:</p> <div style="text-align: center;"> $\nabla f(x) = A^T(AA^T)^{-1}(Ax-y) = A^{\dagger}(Ax-y)$ <br/><br/> </div> <p>The PGM update step is given by:</p> <div style="text-align: center;"> $x_{k+1} = \text{prox}_{h}(x_k - \nabla f(x_k))$, <br/> $x_{k+1} = \text{prox}_{\lambda g}(x_k - A^{\dagger}(Ax_k-y))$. <br/><br/> </div> <p>Notice that the term inside the proximal operator is exactly the projection of $x_k$ onto the solution space $C$:</p> <div style="text-align: center;"> $x_k - A^{\dagger}(Ax_k-y) = \text{proj}_C(x_k)$ <br/><br/> </div> <p>This allows us to write the update in a very intuitive form:</p> <div style="text-align: center;"> $x_{k+1} = \text{prox}_{\lambda g}(\text{prox}_{\iota_C}(x_k))$ <br/><br/> </div> <p class="text-justify">The iterative scheme that follows the proximal gradient method can also be viewed from the perspective of <strong>alternating approximations</strong> <d-cite key="kamath2024method"></d-cite>.</p> <p>Empirically, it was observed that the method of alternating proximations provided faster and superior-quality reconstructions compared to the standard least-squares data-fidelity for compressive sensing <d-cite key="nareddy2024tight"></d-cite> and image deconvolution <d-cite key="nareddy2024image"></d-cite>.</p>]]></content><author><name>Abijith J. Kamath</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Neuromorphic Unlimited Sampling</title><link href="https://spectrum-lab-iisc.github.io/blog/2024/nus/" rel="alternate" type="text/html" title="Neuromorphic Unlimited Sampling"/><published>2024-04-18T00:00:00+00:00</published><updated>2024-04-18T00:00:00+00:00</updated><id>https://spectrum-lab-iisc.github.io/blog/2024/nus</id><content type="html" xml:base="https://spectrum-lab-iisc.github.io/blog/2024/nus/"><![CDATA[<div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nus/nus-banner.jpeg" sizes="95vw"/> <img src="/assets/img/nus/nus-banner.jpeg" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p class="text-justify">Unlimited sampling (US) is a computational sensing paradigm for high-dynamic range (HDR) acquisition of signals. The objective is to overcome the limitations in standard analog-to-digital converters (ADCs) where the dynamic range is fixed: signal that lies outside the fixed dynamic range saturates.</p> <p class="text-justify">Unlimited sampling is an important problem in addressing signal representation in practical systems and has applications in almost all imaging systems. We propose a new technique for unlimited sampling using a neuromorphic encoder. Our system is power efficient, simple to implement in practice, and most importantly, does not require oversampling.</p> <h2 id="computational-modulo-sampling">Computational Modulo Sampling</h2> <p class="text-justify">Computational modulo sampling is a framework for unlimted sampling that uses a self-reset ADC (SR-ADC) as opposed to a standard ADC, where a continuous-time signal \(f(t)\) is first folded using the continuous-domain modulo operator before acquisition. The idea of computational unlimited sampling was first proposed by <a href="http://alumni.media.mit.edu/~ayush/usf.html">Dr Ayush Bhandari</a>, currently at Imperial College London. The continuous-time modulo operator with parameter \(\lambda\) is defined as</p> \[f\mapsto \mathcal{M}_{\lambda}\{f\} = 2\lambda\left[\frac{f}{2\lambda}+\frac{1}{2}\right]-\lambda,\] <p class="text-justify">where \([\cdot]\) defines the fractional part of the argument. The modulo operator restricts the dynamic range to the interval \([-\lambda,+\lambda]\) and enables acquisition of high-dynamic range (in principle, unlimited) acquisition.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0 blog-ready"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nus/mod-encoder.png" sizes="95vw"/> <img src="/assets/img/nus/mod-encoder.png" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 1: What happens in a computational unlimited sampling setup. </div> <p class="text-justify">Reconstruction of the high-dynamic range signal from samples of \(\mathcal{M}_{\lambda}\{f\}\) requires oversampling as compared to the Nyquist rate of \(f\). Further, for practical deployment of unlimited sampling, specialised hardware is required. This can indeed be a practical bottleneck! Oversampling is expensive, and adds to the cost of replacing the existing standard ADC with a self-reset ADC. Our solution addresses this problem and is as simple as using a nonlinear filter before acquisition!</p> <h2 id="the-neuromorphic-unlimited-sampling-framework">The <strong>Neuromorphic Unlimited Sampling</strong> Framework</h2> <p class="text-justify">At the heart of our solution is the neuromorphic encoder, which is a bio-inspired acquisition device that has been successfully deployed in making efficient video acquisition systems. For more on neuromorphic video acquisition, check out the work from the <a href="https://rpg.ifi.uzh.ch/index.html">Robotics and Perception Group</a>.</p> <p class="text-justify">Neuromorphic encoders are opportunistic devices that record signals as a stream of events that denote a change in the input by a constant. The signal is represented as a sequence of 2-tuples, each constituting the time instant and the polarity of change.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0 blog-ready"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nus/nus-encoder.jpeg" sizes="95vw"/> <img src="/assets/img/nus/nus-encoder.jpeg" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 2: Schematic of a neuromorphic encoder. The output is a sequence of 2-tuples characterising the time instant and polarity of change in the absolute-value of the signal by the dynamic-range of the ADC <d-cite key="kamath2023neuromorphic"></d-cite>. </div> <p class="text-justify">Neuromorphic encoders are of practical interest as they do not record any measurements when there is no significant change in the signal. The key observation for unlimited sampling is that the difference signal \(v(t)\) is a folded version of the input signal \(f(t)\), and is completely accommodated in the range \([-\lambda, +\lambda]\). In essence, the signal \(v(t)\) is a polarity dependent modulo operator acting on the input signal as</p> \[f\mapsto v = \mathcal{F}_\lambda\{f\}.\] <p>The in-built folding in the neuromorphic encoder along with its opportunistic nature makes it a perfect fit for unlimited sampling.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0 blog-ready"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nus/nus-schematic.jpeg" sizes="95vw"/> <img src="/assets/img/nus/nus-schematic.jpeg" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 3: Unlimited sampling using a neuromorphic encoder: The accompanying ADC operates at the Nyquist rate and at full precision. The error incurred due to folding is tracked using the neuromorphic encoder allowing real-time reconstruction. </div> <p class="text-justify">Similar to modulo sampling, the error signal \(f(t)-v(t)\) is a piecewise constant signal with amplitude quantised to multiples of \(\lambda\). In computational modulo sampling, the error signal is estimated from <em>oversampled</em> measurements of the folded signal using the repeated finite-difference algorithm. In neuromorphic unlimited sampling, the error signal is captured succinctly using the output of the neuromorphic encoder as</p> \[\eta_f(t) = \sum_m \lambda\mathcal{S}\{p_m\} 1_{[t_m,t_{m+1}]}(t),\] <p class="text-justify">where \(\mathcal{S}\) denotes the cumulative summation operator. Therefore, no oversampling is required! Uniform samples of the folded signal \(v\) captures the high-dynamic-range information in the input signal, achieving unlimited sampling!</p> <p class="text-justify">Additional measurements are made using the neuromorphic encoder opportunistically, i.e., only when the signal goes beyond the dynamic range while consuming much less power than the ADC. We also show that the maximal rate of additional measurements that may be obtained decays with the dynamic range. Our perfect reconstruction strategy only requires addition of a residual signal constructed using the compressed representation of the error signal, which can be achieved in real time.</p> <div class="row justify-content-sm-center"> <div class="col-sm-9 mt-3 mt-md-0 blog-ready"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nus/nus-demo.png" sizes="95vw"/> <img src="/assets/img/nus/nus-demo.png" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 4: Demonstration of neuromorphic sampling and perfect reconstruction. </div> <p class="text-justify">Our patented unlimited sampling hardware using simple and cost-effective components in-house <d-cite key="kulur2024neuromorphic"></d-cite>. The neuromorphic unlimited sampling scheme is such that the neuromorphic encoder can be plugged into an existing acquisition system to achieve unlimited sampling, akin to a prefilter typically used in sampling, and does not require specialised hardware such as the self-reset ADC. In hardware, we demonstrate HDR acquisition and real-time reconstruction of synthetic signals. We show that neuromorphic unlimited sampling operates at the Nyquist rate of the signal, while opportunistically recording compressed measurements. Our demonstration was presented at the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2024, in Seoul Korea 2024, in the Show-and-tell Demos <d-cite key="kulur2024modulo"></d-cite>. A detailed theoretical exposition is available in our paper <d-cite key="kamath2024neuromorphic"></d-cite> which was also presented at IEEE ICASSP 2024. The extension of this theory to a larger class of signals including video signals is going to be presented at the IEEE ICASSP 2025 <d-cite key="kamath2025neuromorphic"></d-cite>.</p> <figure> <video src="/assets/video/nus-demo.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> <p class="text-justify">The hardware contributions to this project are in collaboration with Shreyas, Shreyansh, Satyapreet and Dr Thakur at <a href="https://labs.dese.iisc.ac.in/neuronics/">NeuRonICS, Department of Electronic Systems Engineering</a>, IISc. Bengaluru.</p>]]></content><author><name>Abijith J. Kamath</name></author><summary type="html"><![CDATA[Acquiring high-dynamic range signals with minimal oversampling.]]></summary></entry></feed>